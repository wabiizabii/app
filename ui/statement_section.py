# ui/statement_section.py (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà v2: ‡πÉ‡∏ä‡πâ Active Portfolio ‡∏à‡∏≤‡∏Å Sidebar)

import streamlit as st
import pandas as pd
from core import statement_processor, supabase_handler as db_handler
import hashlib
from datetime import datetime
import time
from config import settings # Ensure settings is imported

def render_statement_section():
    """
    ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Section ‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Statement ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢
    *** ‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î: ‡πÉ‡∏ä‡πâ Active Portfolio ‡∏à‡∏≤‡∏Å Sidebar ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ***
    """
    with st.expander("‚¨ÜÔ∏è Upload Trading Statement", expanded=True):

        # --- 1. ‡∏î‡∏∂‡∏á Active Portfolio ‡∏à‡∏≤‡∏Å session_state ---
        active_portfolio_id = st.session_state.get('active_portfolio_id_gs')
        active_portfolio_name = st.session_state.get('active_portfolio_name_gs')

        if not active_portfolio_id:
            st.warning("‚ö†Ô∏è **‡πÇ‡∏õ‡∏£‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 'Active Portfolio' ‡πÉ‡∏ô‡πÄ‡∏°‡∏ô‡∏π‡∏î‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏Å‡πà‡∏≠‡∏ô** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏û‡∏≠‡∏£‡πå‡∏ï‡πÉ‡∏î")
            return # ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Section ‡∏ô‡∏µ‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ Active Portfolio

        st.success(f"**‡∏Ñ‡∏∏‡∏ì‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏à‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Portfolio:** `{active_portfolio_name}`")
        st.divider()

        # --- 2. ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Statement ---
        st.markdown("##### ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Statement (.csv ‡∏´‡∏£‡∏∑‡∏≠ .html):")
        uploaded_file = st.file_uploader(
            "‡∏•‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏≤‡∏ß‡∏≤‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå",
            type=["csv", "html"],
            label_visibility="collapsed"
        )

        st.divider()

        # --- 3. ‡∏õ‡∏∏‡πà‡∏°‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡πÅ‡∏•‡∏∞ Logic ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ---
        if st.button("üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÉ‡∏ô Portfolio ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å", use_container_width=True, type="primary"):
            
            # --- Logic ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ---
            if not uploaded_file:
                st.warning("‚ö†Ô∏è ‡πÇ‡∏õ‡∏£‡∏î‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Statement ‡∏Å‡πà‡∏≠‡∏ô")
                return # ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ

            with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå..."):
                file_content_bytes = uploaded_file.getvalue()
                file_hash = hashlib.sha256(file_content_bytes).hexdigest()

                # --- Logic ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥ ---
                is_duplicate, duplicate_details = db_handler.check_duplicate_file(file_hash, active_portfolio_id)
                if is_duplicate:
                    st.error(f"‚ùå **‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥!** ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡πÄ‡∏Ñ‡∏¢‡∏ñ‡∏π‡∏Å‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Portfolio '{active_portfolio_name}' ‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏°‡∏∑‡πà‡∏≠ {duplicate_details.get('UploadTimestamp')}")
                    return # ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ

                # --- ‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å ---
                try:
                    # (‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏£)
                    extracted_data = statement_processor.extract_data_from_report_content(file_content_bytes)
                    
                    if not extracted_data or extracted_data.get('deals', pd.DataFrame()).empty:
                        st.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î (Deals) ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ ‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå")
                        return

                    data_to_save = {}
                    import_batch_id = str(int(datetime.now().timestamp()))
                    
                    # Loop through main dataframes
                    for key, table_name in settings.WORKSHEET_HEADERS_MAPPER.items():
                        df = extracted_data.get(key, pd.DataFrame())
                        df_to_save = pd.DataFrame() # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏ß‡πà‡∏≤‡∏á‡πÜ ‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô
                        if not df.empty:
                            df_to_save = df.copy()
                            df_to_save['PortfolioID'] = str(active_portfolio_id)
                            df_to_save['PortfolioName'] = active_portfolio_name
                            df_to_save['SourceFile'] = uploaded_file.name
                            df_to_save['ImportBatchID'] = import_batch_id
                        data_to_save[table_name] = df_to_save
                    
                    # Prepare summary data
                    summary_data = extracted_data.get('final_summary_data', {})
                    if summary_data:
                        summary_data['PortfolioID'] = str(active_portfolio_id)
                        summary_data['PortfolioName'] = active_portfolio_name
                        summary_data['SourceFile'] = uploaded_file.name
                        summary_data['ImportBatchID'] = import_batch_id
                        summary_data['Timestamp'] = datetime.now()
                    data_to_save[settings.SUPABASE_TABLE_STATEMENT_SUMMARIES] = summary_data

                    # Prepare upload history
                    upload_history_data = {
                        "UploadTimestamp": datetime.now(), "PortfolioID": str(active_portfolio_id),
                        "PortfolioName": active_portfolio_name, "FileName": uploaded_file.name,
                        "FileSize": len(file_content_bytes), "FileHash": file_hash, "Status": "Success",
                        "ImportBatchID": import_batch_id, "Notes": "Uploaded via Streamlit app"
                    }
                    data_to_save[settings.SUPABASE_TABLE_UPLOAD_HISTORY] = upload_history_data
                    
                    st.info("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
                    success, message = db_handler.save_statement_data(data_to_save)

                    if success:
                        st.success(f"‚úîÔ∏è ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Portfolio '{active_portfolio_name}' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                        st.balloons()
                        db_handler.clear_all_caches()
                        time.sleep(2)
                        st.rerun()
                    else:
                        st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å: {message}")

                except Exception as e:
                    st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: {e}")